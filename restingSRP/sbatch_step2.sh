#!/bin/bash

#SBATCH --time=3:00:00   # walltime
#SBATCH --ntasks=1   # number of processor cores (i.e. tasks)
#SBATCH --nodes=1   # number of nodes
#SBATCH --mem-per-cpu=4gb   # memory per CPU core
#SBATCH -J "step2"   # job name
#SBATCH --mail-user=ben88@byu.edu  # email address
#SBATCH --mail-type=BEGIN
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL

# Compatibility variables for PBS. Delete if not needed.
export PBS_NODEFILE=`/fslapps/fslutils/generate_pbs_nodefile`
export PBS_JOBID=$SLURM_JOB_ID
export PBS_O_WORKDIR="$SLURM_SUBMIT_DIR"
export PBS_QUEUE=batch

# Set the max number of threads to use for programs using OpenMP. Should be <= ppn. Does nothing if the program doesn't use OpenMP.
export OMP_NUM_THREADS=$SLURM_CPUS_ON_NODE

################
#---PREAMBLE---#
################

# Written by Benjamin Carter 2019-04-27

# Operations: what does this script do? (aka outline of the steps and a minor explanation of why)
# 1. Submits the tcsh script generated by afni_proc.py
#
# Requires: what does this script require to run?
# 1. A script generated by afni_proc.py in step 1.


###################
#---ENVIRONMENT---#
###################

START=$(pwd)
STUDY_DIR=~/compute/skilledReadingStudy
REST_DIR=${STUDY_DIR}/resting
PARTICIPANT=${REST_DIR}/${1}

###############
#---COMMANDS--#
###############
cd ${PARTICIPANT}
tcsh -xef proc.${1} 2>&1 | tee output.proc.${1}


## Example of afni_proc generated script ##

# #!/bin/tcsh -xef
#
# echo "auto-generated by afni_proc.py, Wed May  1 15:45:30 2019"
# echo "(version 6.31, January 30, 2019)"
# echo "execution started: `date`"
#
# # to execute via tcsh:
# #   tcsh -xef proc.Luke_Reading_S11 |& tee output.proc.Luke_Reading_S11
# # to execute via bash:
# #   tcsh -xef proc.Luke_Reading_S11 2>&1 | tee output.proc.Luke_Reading_S11
#
# # =========================== auto block: setup ============================
# # script setup
#
# # take note of the AFNI version
# afni -ver
#
# # check that the current AFNI version is recent enough
# afni_history -check_date 17 Jan 2019
# if ( $status ) then
#     echo "** this script requires newer AFNI binaries (than 17 Jan 2019)"
#     echo "   (consider: @update.afni.binaries -defaults)"
#     exit
# endif
#
# # the user may specify a single subject to run with
# if ( $#argv > 0 ) then
#     set subj = $argv[1]
# else
#     set subj = Luke_Reading_S11
# endif
#
# # assign output directory name
# set output_dir = $subj.results
#
# # verify that the results directory does not yet exist
# if ( -d $output_dir ) then
#     echo output dir "$subj.results" already exists
#     exit
# endif
#
# # set list of runs
# set runs = (`count -digits 2 1 1`)
#
# # create results and stimuli directories
# mkdir $output_dir
# mkdir $output_dir/stimuli
#
# # copy anatomy to results dir
# 3dcopy                                                                                              \
#     /fslhome/ben88/compute/skilledReadingStudy/structural/Luke_Reading_S11/Luke_Reading_S11_T1w.nii \
#     $output_dir/Luke_Reading_S11_T1w
#
# # ============================ auto block: tcat ============================
# # apply 3dTcat to copy input dsets to results dir,
# # while removing the first 3 TRs
# 3dTcat -prefix $output_dir/pb00.$subj.r01.tcat \
#     /fslhome/ben88/compute/skilledReadingStudy/resting/Luke_Reading_S11/Luke_Reading_S11_T2wRest.nii.gz'[3..$]'
#
# # and make note of repetitions (TRs) per run
# set tr_counts = ( 177 )
#
# # -------------------------------------------------------
# # enter the results directory (can begin processing data)
# cd $output_dir
#
#
# # ========================== auto block: outcount ==========================
# # data check: compute outlier fraction for each volume
# touch out.pre_ss_warn.txt
# foreach run ( $runs )
#     3dToutcount -automask -fraction -polort 3 -legendre                     \
#                 pb00.$subj.r$run.tcat+orig > outcount.r$run.1D
#
#     # censor outlier TRs per run, ignoring the first 0 TRs
#     # - censor when more than 0.05 of automask voxels are outliers
#     # - step() defines which TRs to remove via censoring
#     1deval -a outcount.r$run.1D -expr "1-step(a-0.05)" > rm.out.cen.r$run.1D
#
#     # outliers at TR 0 might suggest pre-steady state TRs
#     if ( `1deval -a outcount.r$run.1D"{0}" -expr "step(a-0.4)"` ) then
#         echo "** TR #0 outliers: possible pre-steady state TRs in run $run" \
#             >> out.pre_ss_warn.txt
#     endif
# end
#
# # catenate outlier counts into a single time series
# cat outcount.r*.1D > outcount_rall.1D
#
# # catenate outlier censor files into a single time series
# cat rm.out.cen.r*.1D > outcount_${subj}_censor.1D
#
# # ================================ despike =================================
# # apply 3dDespike to each run
# foreach run ( $runs )
#     3dDespike -NEW -nomask -prefix pb01.$subj.r$run.despike \
#         pb00.$subj.r$run.tcat+orig
# end
#
# # ================================= tshift =================================
# # time shift data so all slice timing is the same
# foreach run ( $runs )
#     3dTshift -tzero 0 -quintic -prefix pb02.$subj.r$run.tshift \
#              pb01.$subj.r$run.despike+orig
# end
#
# # --------------------------------
# # extract volreg registration base
# 3dbucket -prefix vr_base pb02.$subj.r01.tshift+orig"[2]"
#
# # ================================= align ==================================
# # for e2a: compute anat alignment transformation to EPI registration base
# # (new anat will be intermediate, stripped, Luke_Reading_S11_T1w_ns+orig)
# align_epi_anat.py -anat2epi -anat Luke_Reading_S11_T1w+orig \
#        -save_skullstrip -suffix _al_junk                    \
#        -epi vr_base+orig -epi_base 0                        \
#        -epi_strip 3dAutomask                                \
#        -volreg off -tshift off
#
# # ================================== tlrc ==================================
# # warp anatomy to standard space (non-linear warp)
# auto_warp.py -base MNI152_T1_2009c+tlrc -input Luke_Reading_S11_T1w_ns+orig \
#              -skull_strip_input no
#
# # move results up out of the awpy directory
# # (NL-warped anat, affine warp, NL warp)
# # (use typical standard space name for anat)
# # (wildcard is a cheap way to go after any .gz)
# 3dbucket -prefix Luke_Reading_S11_T1w_ns awpy/Luke_Reading_S11_T1w_ns.aw.nii*
# mv awpy/anat.un.aff.Xat.1D .
# mv awpy/anat.un.aff.qw_WARP.nii .
#
# # ================================= volreg =================================
# # align each dset to base volume, to anat, warp to tlrc space
#
# # verify that we have a +tlrc warp dataset
# if ( ! -f Luke_Reading_S11_T1w_ns+tlrc.HEAD ) then
#     echo "** missing +tlrc warp dataset: Luke_Reading_S11_T1w_ns+tlrc.HEAD"
#     exit
# endif
#
# # register and warp
# foreach run ( $runs )
#     # register each volume to the base image
#     3dvolreg -verbose -zpad 1 -base vr_base+orig                              \
#              -1Dfile dfile.r$run.1D -prefix rm.epi.volreg.r$run               \
#              -cubic                                                           \
#              -1Dmatrix_save mat.r$run.vr.aff12.1D                             \
#              pb02.$subj.r$run.tshift+orig
#
#     # create an all-1 dataset to mask the extents of the warp
#     3dcalc -overwrite -a pb02.$subj.r$run.tshift+orig -expr 1                 \
#            -prefix rm.epi.all1
#
#     # catenate volreg/epi2anat/tlrc xforms
#     cat_matvec -ONELINE                                                       \
#                anat.un.aff.Xat.1D                                             \
#                Luke_Reading_S11_T1w_al_junk_mat.aff12.1D -I                   \
#                mat.r$run.vr.aff12.1D > mat.r$run.warp.aff12.1D
#
#     # apply catenated xform: volreg/epi2anat/tlrc/NLtlrc
#     # then apply non-linear standard-space warp
#     3dNwarpApply -master Luke_Reading_S11_T1w_ns+tlrc -dxyz 3                 \
#                  -source pb02.$subj.r$run.tshift+orig                         \
#                  -nwarp "anat.un.aff.qw_WARP.nii mat.r$run.warp.aff12.1D"     \
#                  -prefix rm.epi.nomask.r$run
#
#     # warp the all-1 dataset for extents masking
#     3dNwarpApply -master Luke_Reading_S11_T1w_ns+tlrc -dxyz 3                 \
#                  -source rm.epi.all1+orig                                     \
#                  -nwarp "anat.un.aff.qw_WARP.nii mat.r$run.warp.aff12.1D"     \
#                  -interp cubic                                                \
#                  -ainterp NN -quiet                                           \
#                  -prefix rm.epi.1.r$run
#
#     # make an extents intersection mask of this run
#     3dTstat -min -prefix rm.epi.min.r$run rm.epi.1.r$run+tlrc
# end
#
# # make a single file of registration params
# cat dfile.r*.1D > dfile_rall.1D
#
# # ----------------------------------------
# # create the extents mask: mask_epi_extents+tlrc
# # (this is a mask of voxels that have valid data at every TR)
# # (only 1 run, so just use 3dcopy to keep naming straight)
# 3dcopy rm.epi.min.r01+tlrc mask_epi_extents
#
# # and apply the extents mask to the EPI data
# # (delete any time series with missing data)
# foreach run ( $runs )
#     3dcalc -a rm.epi.nomask.r$run+tlrc -b mask_epi_extents+tlrc               \
#            -expr 'a*b' -prefix pb03.$subj.r$run.volreg
# end
#
# # warp the volreg base EPI dataset to make a final version
# cat_matvec -ONELINE                                                           \
#            anat.un.aff.Xat.1D                                                 \
#            Luke_Reading_S11_T1w_al_junk_mat.aff12.1D -I  >                    \
#            mat.basewarp.aff12.1D
#
# 3dNwarpApply -master Luke_Reading_S11_T1w_ns+tlrc -dxyz 3                     \
#              -source vr_base+orig                                             \
#              -nwarp "anat.un.aff.qw_WARP.nii mat.basewarp.aff12.1D"           \
#              -prefix final_epi_vr_base
#
# # create an anat_final dataset, aligned with stats
# 3dcopy Luke_Reading_S11_T1w_ns+tlrc anat_final.$subj
#
# # record final registration costs
# 3dAllineate -base final_epi_vr_base+tlrc -allcostX                            \
#             -input anat_final.$subj+tlrc |& tee out.allcostX.txt
#
# # -----------------------------------------
# # warp anat follower datasets (non-linear)
# 3dNwarpApply -source Luke_Reading_S11_T1w+orig                                \
#              -master anat_final.$subj+tlrc                                    \
#              -ainterp wsinc5 -nwarp anat.un.aff.qw_WARP.nii anat.un.aff.Xat.1D\
#              -prefix anat_w_skull_warped
#
# # ================================== blur ==================================
# # blur each volume of each run
# foreach run ( $runs )
#     3dmerge -1blur_fwhm 4.0 -doall -prefix pb04.$subj.r$run.blur \
#             pb03.$subj.r$run.volreg+tlrc
# end
#
# # ================================== mask ==================================
# # create 'full_mask' dataset (union mask)
# foreach run ( $runs )
#     3dAutomask -prefix rm.mask_r$run pb04.$subj.r$run.blur+tlrc
# end
#
# # create union of inputs, output type is byte
# 3dmask_tool -inputs rm.mask_r*+tlrc.HEAD -union -prefix full_mask.$subj
#
# # ---- create subject anatomy mask, mask_anat.$subj+tlrc ----
# #      (resampled from tlrc anat)
# 3dresample -master full_mask.$subj+tlrc -input Luke_Reading_S11_T1w_ns+tlrc \
#            -prefix rm.resam.anat
#
# # convert to binary anat mask; fill gaps and holes
# 3dmask_tool -dilate_input 5 -5 -fill_holes -input rm.resam.anat+tlrc        \
#             -prefix mask_anat.$subj
#
# # compute tighter EPI mask by intersecting with anat mask
# 3dmask_tool -input full_mask.$subj+tlrc mask_anat.$subj+tlrc                \
#             -inter -prefix mask_epi_anat.$subj
#
# # compute overlaps between anat and EPI masks
# 3dABoverlap -no_automask full_mask.$subj+tlrc mask_anat.$subj+tlrc          \
#             |& tee out.mask_ae_overlap.txt
#
# # note Dice coefficient of masks, as well
# 3ddot -dodice full_mask.$subj+tlrc mask_anat.$subj+tlrc                     \
#       |& tee out.mask_ae_dice.txt
#
# # ---- create group anatomy mask, mask_group+tlrc ----
# #      (resampled from tlrc base anat, MNI152_T1_2009c+tlrc)
# 3dresample -master full_mask.$subj+tlrc -prefix ./rm.resam.group            \
#            -input /fslhome/ben88/abin/MNI152_T1_2009c+tlrc
#
# # convert to binary group mask; fill gaps and holes
# 3dmask_tool -dilate_input 5 -5 -fill_holes -input rm.resam.group+tlrc       \
#             -prefix mask_group
#
# # ================================= scale ==================================
# # scale each voxel time series to have a mean of 100
# # (be sure no negatives creep in)
# # (subject to a range of [0,200])
# foreach run ( $runs )
#     3dTstat -prefix rm.mean_r$run pb04.$subj.r$run.blur+tlrc
#     3dcalc -a pb04.$subj.r$run.blur+tlrc -b rm.mean_r$run+tlrc \
#            -c mask_epi_extents+tlrc                            \
#            -expr 'c * min(200, a/b*100)*step(a)*step(b)'       \
#            -prefix pb05.$subj.r$run.scale
# end
#
# # ================================ regress =================================
#
# # compute de-meaned motion parameters (for use in regression)
# 1d_tool.py -infile dfile_rall.1D -set_nruns 1                            \
#            -demean -write motion_demean.1D
#
# # compute motion parameter derivatives (for use in regression)
# 1d_tool.py -infile dfile_rall.1D -set_nruns 1                            \
#            -derivative -demean -write motion_deriv.1D
#
# # create censor file motion_${subj}_censor.1D, for censoring motion
# 1d_tool.py -infile dfile_rall.1D -set_nruns 1                            \
#     -show_censor_count -censor_prev_TR                                   \
#     -censor_motion 0.2 motion_${subj}
#
# # combine multiple censor files
# 1deval -a motion_${subj}_censor.1D -b outcount_${subj}_censor.1D         \
#        -expr "a*b" > censor_${subj}_combined_2.1D
#
# # create bandpass regressors (instead of using 3dBandpass, say)
# 1dBport -nodata 177 2.0 -band 0.01 0.1 -invert -nozero > bandpass_rall.1D
#
# # note TRs that were not censored
# set ktrs = `1d_tool.py -infile censor_${subj}_combined_2.1D              \
#                        -show_trs_uncensored encoded`
#
# # ------------------------------
# # run the regression analysis
# 3dDeconvolve -input pb05.$subj.r*.scale+tlrc.HEAD                        \
#     -censor censor_${subj}_combined_2.1D                                 \
#     -ortvec bandpass_rall.1D bandpass                                    \
#     -ortvec motion_demean.1D mot_demean                                  \
#     -ortvec motion_deriv.1D mot_deriv                                    \
#     -polort 3                                                            \
#     -num_stimts 0                                                        \
#     -fout -tout -x1D X.xmat.1D -xjpeg X.jpg                              \
#     -x1D_uncensored X.nocensor.xmat.1D                                   \
#     -fitts fitts.$subj                                                   \
#     -errts errts.${subj}                                                 \
#     -x1D_stop                                                            \
#     -bucket stats.$subj
#
# # -- use 3dTproject to project out regression matrix --
# 3dTproject -polort 0 -input pb05.$subj.r*.scale+tlrc.HEAD                \
#            -censor censor_${subj}_combined_2.1D -cenmode ZERO            \
#            -ort X.nocensor.xmat.1D -prefix errts.${subj}.tproject
#
#
#
# # if 3dDeconvolve fails, terminate the script
# if ( $status != 0 ) then
#     echo '---------------------------------------'
#     echo '** 3dDeconvolve error, failing...'
#     echo '   (consider the file 3dDeconvolve.err)'
#     exit
# endif
#
#
# # display any large pairwise correlations from the X-matrix
# 1d_tool.py -show_cormat_warnings -infile X.xmat.1D |& tee out.cormat_warn.txt
#
# # display degrees of freedom info from X-matrix
# 1d_tool.py -show_df_info -infile X.xmat.1D |& tee out.df_info.txt
#
# # create an all_runs dataset to match the fitts, errts, etc.
# 3dTcat -prefix all_runs.$subj pb05.$subj.r*.scale+tlrc.HEAD
#
# # --------------------------------------------------
# # create a temporal signal to noise ratio dataset
# #    signal: if 'scale' block, mean should be 100
# #    noise : compute standard deviation of errts
# 3dTstat -mean -prefix rm.signal.all all_runs.$subj+tlrc"[$ktrs]"
# 3dTstat -stdev -prefix rm.noise.all errts.${subj}.tproject+tlrc"[$ktrs]"
# 3dcalc -a rm.signal.all+tlrc                                             \
#        -b rm.noise.all+tlrc                                              \
#        -c mask_epi_anat.$subj+tlrc                                       \
#        -expr 'c*a/b' -prefix TSNR.$subj
#
# # ---------------------------------------------------
# # compute and store GCOR (global correlation average)
# # (sum of squares of global mean of unit errts)
# 3dTnorm -norm2 -prefix rm.errts.unit errts.${subj}.tproject+tlrc
# 3dmaskave -quiet -mask full_mask.$subj+tlrc rm.errts.unit+tlrc           \
#           > gmean.errts.unit.1D
# 3dTstat -sos -prefix - gmean.errts.unit.1D\' > out.gcor.1D
# echo "-- GCOR = `cat out.gcor.1D`"
#
# # ---------------------------------------------------
# # compute correlation volume
# # (per voxel: average correlation across masked brain)
# # (now just dot product with average unit time series)
# 3dcalc -a rm.errts.unit+tlrc -b gmean.errts.unit.1D -expr 'a*b' -prefix rm.DP
# 3dTstat -sum -prefix corr_brain rm.DP+tlrc
#
# # --------------------------------------------------------
# # compute sum of non-baseline regressors from the X-matrix
# # (use 1d_tool.py to get list of regressor colums)
# set reg_cols = `1d_tool.py -infile X.nocensor.xmat.1D -show_indices_interest`
# 3dTstat -sum -prefix sum_ideal.1D X.nocensor.xmat.1D"[$reg_cols]"
#
# # also, create a stimulus-only X-matrix, for easy review
# 1dcat X.nocensor.xmat.1D"[$reg_cols]" > X.stim.xmat.1D
#
# # ============================ blur estimation =============================
# # compute blur estimates
# touch blur_est.$subj.1D   # start with empty file
#
# # create directory for ACF curve files
# mkdir files_ACF
#
# # -- estimate blur for each run in epits --
# touch blur.epits.1D
#
# # restrict to uncensored TRs, per run
# foreach run ( $runs )
#     set trs = `1d_tool.py -infile X.xmat.1D -show_trs_uncensored encoded \
#                           -show_trs_run $run`
#     if ( $trs == "" ) continue
#     3dFWHMx -detrend -mask mask_epi_anat.$subj+tlrc                      \
#             -ACF files_ACF/out.3dFWHMx.ACF.epits.r$run.1D                \
#             all_runs.$subj+tlrc"[$trs]" >> blur.epits.1D
# end
#
# # compute average FWHM blur (from every other row) and append
# set blurs = ( `3dTstat -mean -prefix - blur.epits.1D'{0..$(2)}'\'` )
# echo average epits FWHM blurs: $blurs
# echo "$blurs   # epits FWHM blur estimates" >> blur_est.$subj.1D
#
# # compute average ACF blur (from every other row) and append
# set blurs = ( `3dTstat -mean -prefix - blur.epits.1D'{1..$(2)}'\'` )
# echo average epits ACF blurs: $blurs
# echo "$blurs   # epits ACF blur estimates" >> blur_est.$subj.1D
#
# # -- estimate blur for each run in errts --
# touch blur.errts.1D
#
# # restrict to uncensored TRs, per run
# foreach run ( $runs )
#     set trs = `1d_tool.py -infile X.xmat.1D -show_trs_uncensored encoded \
#                           -show_trs_run $run`
#     if ( $trs == "" ) continue
#     3dFWHMx -detrend -mask mask_epi_anat.$subj+tlrc                      \
#             -ACF files_ACF/out.3dFWHMx.ACF.errts.r$run.1D                \
#             errts.${subj}.tproject+tlrc"[$trs]" >> blur.errts.1D
# end
#
# # compute average FWHM blur (from every other row) and append
# set blurs = ( `3dTstat -mean -prefix - blur.errts.1D'{0..$(2)}'\'` )
# echo average errts FWHM blurs: $blurs
# echo "$blurs   # errts FWHM blur estimates" >> blur_est.$subj.1D
#
# # compute average ACF blur (from every other row) and append
# set blurs = ( `3dTstat -mean -prefix - blur.errts.1D'{1..$(2)}'\'` )
# echo average errts ACF blurs: $blurs
# echo "$blurs   # errts ACF blur estimates" >> blur_est.$subj.1D
#
#
# # ================== auto block: generate review scripts ===================
#
# # generate a review script for the unprocessed EPI data
# gen_epi_review.py -script @epi_review.$subj             \
#     -dsets pb00.$subj.r*.tcat+orig.HEAD
#
# # generate scripts to review single subject results
# # (try with defaults, but do not allow bad exit status)
# gen_ss_review_scripts.py -mot_limit 0.2 -out_limit 0.05 \
#     -errts_dset errts.${subj}.tproject+tlrc.HEAD -exit0 \
#     -ss_review_dset out.ss_review.$subj.txt             \
#     -write_uvars_json out.ss_review_uvars.json
#
# # ========================== auto block: finalize ==========================
#
# # remove temporary files
# \rm -fr rm.* awpy
#
# # if the basic subject review script is here, run it
# # (want this to be the last text output)
# if ( -e @ss_review_basic ) then
#     ./@ss_review_basic |& tee out.ss_review.$subj.txt
#
# endif
#
# # return to parent directory (just in case...)
# cd ..
#
# echo "execution finished: `date`"
#
#
#
#
# # ==========================================================================
# # script generated by the command:
# #
# # afni_proc.py -subj_id Luke_Reading_S11 -dsets                                                           \
# #     /fslhome/ben88/compute/skilledReadingStudy/resting/Luke_Reading_S11/Luke_Reading_S11_T2wRest.nii.gz \
# #     -copy_anat                                                                                          \
# #     /fslhome/ben88/compute/skilledReadingStudy/structural/Luke_Reading_S11/Luke_Reading_S11_T1w.nii     \
# #     -blocks despike tshift align tlrc volreg blur mask scale regress                                    \
# #     -tcat_remove_first_trs 3 -tlrc_base MNI152_T1_2009c+tlrc -tlrc_NL_warp                              \
# #     -volreg_align_e2a -volreg_tlrc_warp -mask_epi_anat yes                                              \
# #     -regress_censor_motion 0.2 -regress_censor_outliers 0.05                                            \
# #     -regress_bandpass 0.01 0.1 -regress_apply_mot_types demean deriv                                    \
# #     -regress_est_blur_epits -regress_est_blur_errts
